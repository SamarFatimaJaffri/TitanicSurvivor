{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Titanic Survivors Prediction"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Engineering\n",
    "\n",
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cuMHKyrhoqQG",
    "outputId": "3cbe3578-caa4-4324-a625-4ee6e2ee8a21"
   },
   "source": [
    "import pandas as pd\n",
    "from pyexpat import features\n",
    "\n",
    "df = pd.read_csv('data/titanic_dataset.csv')\n",
    "print(f'Dataset shape: {df.shape}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df.head(5)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "zYQjNMkbrWaS",
    "outputId": "587f0a93-60f2-4972-ef4e-a9caef56fd8b"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Cleaning"
  },
  {
   "cell_type": "code",
   "source": [
    "# check for null values\n",
    "print(df.isnull().sum())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qrbDbilnrcsw",
    "outputId": "7c8d0f80-b836-444b-d323-8c71b6d08902"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# fill nulls in \"Embarked\" with mode\n",
    "df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])"
   ],
   "metadata": {
    "id": "duFEJ3Z0sC7h"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# recheck null values in \"Embarked\"\n",
    "print(df['Embarked'].isnull().sum())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mQ5jYf0vsTmI",
    "outputId": "96b44601-a7ce-4531-aa33-90be83198769"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "The categorical features e.g., \"sex\" is already encoded in our dataset. If it wasn't than we would have encoded it using label encoding, or one-hot encoding if there were other categorical features based on whether they were ordinal or non-ordinal data.",
   "metadata": {
    "id": "i7aTqqAIsxbB"
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Feature Selection\n",
    "\n",
    "As we know that there are multiple columns which only contains zero, and will have no impact on predictions. We are selecting only non-zero columns for modeling."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# get all non-null columns\n",
    "nonnull_cols = df.columns[~(df == 0).all()]\n",
    "\n",
    "display(df[nonnull_cols].head())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# select important feature for modeling\n",
    "features_ = ['Age', 'Fare', 'Sex', 'sibsp', 'Parch', 'Pclass', 'Embarked']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Train/Test Split\n",
    "\n",
    "We are splitting the data into training and testing datasets. Keeping 20% as test dataset and 80% as train dataset."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[features_]\n",
    "y = df['2urvived']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'y_train shape: {y_train.shape}')\n",
    "print(f'\\nX_test shape: {X_test.shape}')\n",
    "print(f'y_test shape: {y_test.shape}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PQRGlNmOuByo",
    "outputId": "707754d2-423b-48a2-a3ea-708a8e4ef59c"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b165ddea"
   },
   "source": [
    "Perform hyperparameter tuning for a Decision Tree Classifier on the Titanic dataset, utilizing the `/content/train_and_test2.csv` file. The process involves importing necessary `sklearn` libraries, defining a parameter grid for `DecisionTreeClassifier`, using `GridSearchCV` to find optimal hyperparameters on the training data (`X_train`, `y_train`), training a `DecisionTreeClassifier` with these best hyperparameters, and finally evaluating its performance on the test set (`X_test`, `y_test`). The final output should include the best hyperparameters found and the model's performance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6424e52"
   },
   "source": [
    "## Modeling\n",
    "### Import Libraries\n",
    "\n",
    "Import `DecisionTreeClassifier` for the model and `GridSearchCV` for hyperparameter tuning from `sklearn`.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "69949b95",
    "outputId": "b2369de2-0704-4b6d-a328-529ed60e7dc4"
   },
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "print('DecisionTreeClassifier and GridSearchCV imported successfully.')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9af6c20c"
   },
   "source": [
    "### Define Parameter Grid\n",
    "\n",
    "Create a dictionary specifying the hyperparameters and their potential values to be searched during tuning for the Decision Tree.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1bc94bf9",
    "outputId": "ad637d91-7bbe-43cc-8bf9-4e2c8b658545"
   },
   "source": [
    "param_grid = {\n",
    "    'max_depth': [None, 3, 5, 10, 15, 20],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "print('Parameter grid defined successfully.')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "390dc005"
   },
   "source": [
    "### Perform Hyperparameter Tuning\n",
    "\n",
    "Utilize GridSearchCV to systematically search through the defined parameter grid and find the optimal hyperparameters for the Decision Tree Classifier using the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0de15f5f",
    "outputId": "24e6843f-dc58-4940-bc82-b79f0bb71ddb"
   },
   "source": [
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=dt_classifier, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "print('Fitting GridSearchCV to the training data...')\n",
    "grid_search.fit(X_train, y_train)\n",
    "print('GridSearchCV fitting complete.')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fda6f636"
   },
   "source": [
    "Now that GridSearchCV has completed fitting, I need to retrieve and display the best hyperparameters found and the corresponding best score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b4194faa",
    "outputId": "33bc4eb0-714a-44d4-a18f-433c22591f2b"
   },
   "source": [
    "print(f'Best hyperparameters: {grid_search.best_params_}')\n",
    "print(f'Best accuracy score: {grid_search.best_score_:.4f}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3720d10"
   },
   "source": [
    "## Train Final Model and Evaluate\n",
    "\n",
    "Train a `DecisionTreeClassifier` using the best hyperparameters found by `GridSearchCV` and evaluate its performance on the test set (`X_test`, `y_test`)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dc9d4068",
    "outputId": "84cf7163-a0fe-48d1-fa5e-32a8aabd0332"
   },
   "source": [
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, classification_report)\n",
    "\n",
    "# Get the best hyperparameters from GridSearchCV\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Initialize a new DecisionTreeClassifier with the best hyperparameters\n",
    "final_dt_model = DecisionTreeClassifier(**best_params, random_state=42)\n",
    "\n",
    "# Train the final model on the entire training dataset\n",
    "print('Training final Decision Tree model...')\n",
    "final_dt_model.fit(X_train, y_train)\n",
    "print('Final model training complete.')\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = final_dt_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print('\\nModel Evaluation on Test Set:')\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred):.4f}')\n",
    "print(f'Precision: {precision_score(y_test, y_pred):.4f}')\n",
    "print(f'Recall: {recall_score(y_test, y_pred):.4f}')\n",
    "print(f'F1-Score: {f1_score(y_test, y_pred):.4f}')\n",
    "\n",
    "print('\\nClassification Report:\\n', classification_report(y_test, y_pred))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Confusion Matrix\n"
   ],
   "metadata": {
    "id": "aWuHo2g70_6N"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aOgiTQ6fx5p2",
    "outputId": "667131d7-706c-40e0-e783-26249d52c1a6"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The confusion matrix above indicates the following,  \n",
    "- True/Positive: 0\n",
    "- False/Negative: 1"
   ],
   "metadata": {
    "id": "L9z8jj4q1XmJ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Decision Tree"
   ],
   "metadata": {
    "id": "rhbYcca21Oyj"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "49d3f25b",
    "outputId": "31d42d7e-c37c-492d-ac79-bc1f54aa0b08"
   },
   "source": [
    "feature_imp = pd.Series(final_dt_model.feature_importances_, index=X.columns)\n",
    "top_3_features = feature_imp.nlargest(3)\n",
    "\n",
    "print('Top 3 most important features:')\n",
    "print(top_3_features)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b87515ce"
   },
   "source": [
    "### How Decision Trees Split Data\n",
    "\n",
    "Decision trees work by recursively partitioning the data into subsets based on the values of the input features. At each node of the tree, the algorithm selects the feature and the split point that best divides the data into distinct groups, aiming to maximize the 'purity' of the resulting subsets with respect to the target variable.\n",
    "\n",
    "In our case, the `criterion` for splitting was 'entropy', meaning the tree aims to reduce the impurity (randomness) in the target variable as much as possible at each split. The tree iteratively asks 'if-else' questions about the features, such as 'Is Age <= 28.5?', leading to branches that eventually terminate in leaf nodes, each representing a predicted outcome (survived or not survived).\n",
    "\n",
    "### Top 3 Most Important Features\n",
    "\n",
    "Based on the feature importances from our `final_dt_model`, the top 3 most influential features in determining survival on the Titanic dataset are:\n",
    "\n",
    "1.  **Sex**: The gender of the passenger.\n",
    "2.  **Fare**: The fare paid by the passenger.\n",
    "3.  **Age**: The age of the passenger.\n",
    "\n",
    "These features are used at the higher levels of the decision tree to make the most impactful initial splits, contributing significantly to the model's predictive power."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn import tree\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "tree.plot_tree(\n",
    "    final_dt_model, filled=True, feature_names=features_, class_names=[str(c) for c in y.unique()]\n",
    ")\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "X2Aw6jzqzsS4",
    "outputId": "f3e24622-f7d9-447b-8a79-a32a0b79429e"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
